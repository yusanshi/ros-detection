diff --git a/lidar_point_pillars/nodes/point_pillars_ros.cpp b/lidar_point_pillars/nodes/point_pillars_ros.cpp
index 776ce59..ac0886e 100644
--- a/lidar_point_pillars/nodes/point_pillars_ros.cpp
+++ b/lidar_point_pillars/nodes/point_pillars_ros.cpp
@@ -19,8 +19,8 @@
 #include <cmath>
 
 // headers in PCL
-#include <pcl_conversions/pcl_conversions.h>
 #include <pcl/PCLPointCloud2.h>
+#include <pcl_conversions/pcl_conversions.h>
 #include <pcl_ros/transforms.h>
 
 // headers in ROS
@@ -31,152 +31,154 @@
 #include "lidar_point_pillars/point_pillars_ros.h"
 
 PointPillarsROS::PointPillarsROS()
-  : private_nh_("~")
-  , has_subscribed_baselink_(false)
-  , NUM_POINT_FEATURE_(4)
-  , OUTPUT_NUM_BOX_FEATURE_(7)
-  , TRAINED_SENSOR_HEIGHT_(1.73f)
-  , NORMALIZING_INTENSITY_VALUE_(255.0f)
-  , BASELINK_FRAME_("base_link")
-{
-  //ros related param
-  private_nh_.param<bool>("baselink_support", baselink_support_, true);
-
-  //algorithm related params
-  private_nh_.param<bool>("reproduce_result_mode", reproduce_result_mode_, false);
-  private_nh_.param<float>("score_threshold", score_threshold_, 0.5f);
-  private_nh_.param<float>("nms_overlap_threshold", nms_overlap_threshold_, 0.5f);
-  private_nh_.param<std::string>("pfe_onnx_file", pfe_onnx_file_, "");
-  private_nh_.param<std::string>("rpn_onnx_file", rpn_onnx_file_, "");
-
-  point_pillars_ptr_.reset(new PointPillars(reproduce_result_mode_, score_threshold_, nms_overlap_threshold_,
-                                            pfe_onnx_file_, rpn_onnx_file_));
+    : private_nh_("~"),
+      has_subscribed_baselink_(false),
+      NUM_POINT_FEATURE_(4),
+      OUTPUT_NUM_BOX_FEATURE_(7),
+      TRAINED_SENSOR_HEIGHT_(1.73f),
+      NORMALIZING_INTENSITY_VALUE_(255.0f),
+      BASELINK_FRAME_("base_link") {
+    // ros related param
+    private_nh_.param<bool>("baselink_support", baselink_support_, true);
+
+    // algorithm related params
+    private_nh_.param<bool>("reproduce_result_mode", reproduce_result_mode_,
+                            false);
+    private_nh_.param<float>("score_threshold_point_pillars", score_threshold_,
+                             0.5f);
+    private_nh_.param<float>("nms_overlap_threshold", nms_overlap_threshold_,
+                             0.5f);
+    private_nh_.param<std::string>("pfe_onnx_file", pfe_onnx_file_, "");
+    private_nh_.param<std::string>("rpn_onnx_file", rpn_onnx_file_, "");
+
+    point_pillars_ptr_.reset(new PointPillars(
+        reproduce_result_mode_, score_threshold_, nms_overlap_threshold_,
+        pfe_onnx_file_, rpn_onnx_file_));
 }
 
-void PointPillarsROS::createROSPubSub()
-{
-  sub_points_ = nh_.subscribe<sensor_msgs::PointCloud2>("/points_raw", 1, &PointPillarsROS::pointsCallback, this);
-  pub_objects_ = nh_.advertise<autoware_msgs::DetectedObjectArray>("/detection/lidar_detector/objects", 1);
+void PointPillarsROS::createROSPubSub() {
+    std::string input_points_str = "/input_points";
+    std::string output_objects_str = "/output_objects";
+    sub_points_ = nh_.subscribe<sensor_msgs::PointCloud2>(
+        input_points_str, 1, &PointPillarsROS::pointsCallback, this);
+    pub_objects_ = nh_.advertise<autoware_msgs::DetectedObjectArray>(
+        output_objects_str, 1);
 }
 
-geometry_msgs::Pose PointPillarsROS::getTransformedPose(const geometry_msgs::Pose& in_pose, const tf::Transform& tf)
-{
-  tf::Transform transform;
-  geometry_msgs::PoseStamped out_pose;
-  transform.setOrigin(tf::Vector3(in_pose.position.x, in_pose.position.y, in_pose.position.z));
-  transform.setRotation(
-      tf::Quaternion(in_pose.orientation.x, in_pose.orientation.y, in_pose.orientation.z, in_pose.orientation.w));
-  geometry_msgs::PoseStamped pose_out;
-  tf::poseTFToMsg(tf * transform, out_pose.pose);
-  return out_pose.pose;
+geometry_msgs::Pose PointPillarsROS::getTransformedPose(
+    const geometry_msgs::Pose& in_pose, const tf::Transform& tf) {
+    tf::Transform transform;
+    geometry_msgs::PoseStamped out_pose;
+    transform.setOrigin(tf::Vector3(in_pose.position.x, in_pose.position.y,
+                                    in_pose.position.z));
+    transform.setRotation(
+        tf::Quaternion(in_pose.orientation.x, in_pose.orientation.y,
+                       in_pose.orientation.z, in_pose.orientation.w));
+    geometry_msgs::PoseStamped pose_out;
+    tf::poseTFToMsg(tf * transform, out_pose.pose);
+    return out_pose.pose;
 }
 
-void PointPillarsROS::pubDetectedObject(const std::vector<float>& detections, const std_msgs::Header& in_header)
-{
-  autoware_msgs::DetectedObjectArray objects;
-  objects.header = in_header;
-  int num_objects = detections.size() / OUTPUT_NUM_BOX_FEATURE_;
-  for (size_t i = 0; i < num_objects; i++)
-  {
-    autoware_msgs::DetectedObject object;
-    object.header = in_header;
-    object.valid = true;
-    object.pose_reliable = true;
-
-    object.pose.position.x = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 0];
-    object.pose.position.y = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 1];
-    object.pose.position.z = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 2];
-
-    // Trained this way
-    float yaw = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 6];
-    yaw += M_PI/2;
-    yaw = std::atan2(std::sin(yaw), std::cos(yaw));
-    geometry_msgs::Quaternion q = tf::createQuaternionMsgFromYaw(-yaw);
-    object.pose.orientation = q;
-
-    if (baselink_support_)
-    {
-      object.pose = getTransformedPose(object.pose, angle_transform_inversed_);
+void PointPillarsROS::pubDetectedObject(const std::vector<float>& detections,
+                                        const std_msgs::Header& in_header) {
+    autoware_msgs::DetectedObjectArray objects;
+    objects.header = in_header;
+    int num_objects = detections.size() / OUTPUT_NUM_BOX_FEATURE_;
+    for (size_t i = 0; i < num_objects; i++) {
+        autoware_msgs::DetectedObject object;
+        object.header = in_header;
+        object.valid = true;
+        object.pose_reliable = true;
+
+        object.pose.position.x = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 0];
+        object.pose.position.y = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 1];
+        object.pose.position.z = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 2];
+
+        // Trained this way
+        float yaw = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 6];
+        yaw += M_PI / 2;
+        yaw = std::atan2(std::sin(yaw), std::cos(yaw));
+        geometry_msgs::Quaternion q = tf::createQuaternionMsgFromYaw(-yaw);
+        object.pose.orientation = q;
+
+        if (baselink_support_) {
+            object.pose =
+                getTransformedPose(object.pose, angle_transform_inversed_);
+        }
+
+        // Again: Trained this way
+        object.dimensions.x = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 4];
+        object.dimensions.y = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 3];
+        object.dimensions.z = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 5];
+
+        // Only detects car in Version 1.0
+        object.label = "car";
+
+        objects.objects.push_back(object);
     }
-
-    // Again: Trained this way
-    object.dimensions.x = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 4];
-    object.dimensions.y = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 3];
-    object.dimensions.z = detections[i * OUTPUT_NUM_BOX_FEATURE_ + 5];
-
-    //Only detects car in Version 1.0
-    object.label = "car";
-
-    objects.objects.push_back(object);
-  }
-  pub_objects_.publish(objects);
+    pub_objects_.publish(objects);
 }
 
-void PointPillarsROS::getBaselinkToLidarTF(const std::string& target_frameid)
-{
-  try
-  {
-    tf_listener_.waitForTransform(BASELINK_FRAME_, target_frameid, ros::Time(0), ros::Duration(1.0));
-    tf_listener_.lookupTransform(BASELINK_FRAME_, target_frameid, ros::Time(0), baselink2lidar_);
-    analyzeTFInfo(baselink2lidar_);
-    has_subscribed_baselink_ = true;
-  }
-  catch (tf::TransformException ex)
-  {
-    ROS_ERROR("%s", ex.what());
-  }
+void PointPillarsROS::getBaselinkToLidarTF(const std::string& target_frameid) {
+    try {
+        tf_listener_.waitForTransform(BASELINK_FRAME_, target_frameid,
+                                      ros::Time(0), ros::Duration(1.0));
+        tf_listener_.lookupTransform(BASELINK_FRAME_, target_frameid,
+                                     ros::Time(0), baselink2lidar_);
+        analyzeTFInfo(baselink2lidar_);
+        has_subscribed_baselink_ = true;
+    } catch (tf::TransformException ex) {
+        ROS_ERROR("%s", ex.what());
+    }
 }
 
-void PointPillarsROS::analyzeTFInfo(tf::StampedTransform baselink2lidar)
-{
-  tf::Vector3 v = baselink2lidar.getOrigin();
-  offset_z_from_trained_data_ = v.getZ() - TRAINED_SENSOR_HEIGHT_;
+void PointPillarsROS::analyzeTFInfo(tf::StampedTransform baselink2lidar) {
+    tf::Vector3 v = baselink2lidar.getOrigin();
+    offset_z_from_trained_data_ = v.getZ() - TRAINED_SENSOR_HEIGHT_;
 
-  tf::Quaternion q = baselink2lidar_.getRotation();
-  angle_transform_ = tf::Transform(q);
-  angle_transform_inversed_ = angle_transform_.inverse();
+    tf::Quaternion q = baselink2lidar_.getRotation();
+    angle_transform_ = tf::Transform(q);
+    angle_transform_inversed_ = angle_transform_.inverse();
 }
 
-void PointPillarsROS::pclToArray(const pcl::PointCloud<pcl::PointXYZI>::Ptr& in_pcl_pc_ptr, float* out_points_array,
-                                 const float offset_z)
-{
-  for (size_t i = 0; i < in_pcl_pc_ptr->size(); i++)
-  {
-    pcl::PointXYZI point = in_pcl_pc_ptr->at(i);
-    out_points_array[i * NUM_POINT_FEATURE_ + 0] = point.x;
-    out_points_array[i * NUM_POINT_FEATURE_ + 1] = point.y;
-    out_points_array[i * NUM_POINT_FEATURE_ + 2] = point.z + offset_z;
-    out_points_array[i * NUM_POINT_FEATURE_ + 3] = float(point.intensity / NORMALIZING_INTENSITY_VALUE_);
-  }
+void PointPillarsROS::pclToArray(
+    const pcl::PointCloud<pcl::PointXYZI>::Ptr& in_pcl_pc_ptr,
+    float* out_points_array, const float offset_z) {
+    for (size_t i = 0; i < in_pcl_pc_ptr->size(); i++) {
+        pcl::PointXYZI point = in_pcl_pc_ptr->at(i);
+        out_points_array[i * NUM_POINT_FEATURE_ + 0] = point.x;
+        out_points_array[i * NUM_POINT_FEATURE_ + 1] = point.y;
+        out_points_array[i * NUM_POINT_FEATURE_ + 2] = point.z + offset_z;
+        out_points_array[i * NUM_POINT_FEATURE_ + 3] =
+            float(point.intensity / NORMALIZING_INTENSITY_VALUE_);
+    }
 }
 
-void PointPillarsROS::pointsCallback(const sensor_msgs::PointCloud2::ConstPtr& msg)
-{
-  pcl::PointCloud<pcl::PointXYZI>::Ptr pcl_pc_ptr(new pcl::PointCloud<pcl::PointXYZI>);
-  pcl::fromROSMsg(*msg, *pcl_pc_ptr);
+void PointPillarsROS::pointsCallback(
+    const sensor_msgs::PointCloud2::ConstPtr& msg) {
+    pcl::PointCloud<pcl::PointXYZI>::Ptr pcl_pc_ptr(
+        new pcl::PointCloud<pcl::PointXYZI>);
+    pcl::fromROSMsg(*msg, *pcl_pc_ptr);
+
+    if (baselink_support_) {
+        if (!has_subscribed_baselink_) {
+            getBaselinkToLidarTF(msg->header.frame_id);
+        }
+        pcl_ros::transformPointCloud(*pcl_pc_ptr, *pcl_pc_ptr,
+                                     angle_transform_);
+    }
 
-  if (baselink_support_)
-  {
-    if (!has_subscribed_baselink_)
-    {
-      getBaselinkToLidarTF(msg->header.frame_id);
+    float* points_array = new float[pcl_pc_ptr->size() * NUM_POINT_FEATURE_];
+    if (baselink_support_ && has_subscribed_baselink_) {
+        pclToArray(pcl_pc_ptr, points_array, offset_z_from_trained_data_);
+    } else {
+        pclToArray(pcl_pc_ptr, points_array);
     }
-    pcl_ros::transformPointCloud(*pcl_pc_ptr, *pcl_pc_ptr, angle_transform_);
-  }
-
-  float* points_array = new float[pcl_pc_ptr->size() * NUM_POINT_FEATURE_];
-  if (baselink_support_ && has_subscribed_baselink_)
-  {
-    pclToArray(pcl_pc_ptr, points_array, offset_z_from_trained_data_);
-  }
-  else
-  {
-    pclToArray(pcl_pc_ptr, points_array);
-  }
-
-  std::vector<float> out_detection;
-  point_pillars_ptr_->doInference(points_array, pcl_pc_ptr->size(), out_detection);
-
-  delete[] points_array;
-  pubDetectedObject(out_detection, msg->header);
+
+    std::vector<float> out_detection;
+    point_pillars_ptr_->doInference(points_array, pcl_pc_ptr->size(),
+                                    out_detection);
+
+    delete[] points_array;
+    pubDetectedObject(out_detection, msg->header);
 }
diff --git a/range_vision_fusion/src/range_vision_fusion.cpp b/range_vision_fusion/src/range_vision_fusion.cpp
index bc46c80..9122c9e 100644
--- a/range_vision_fusion/src/range_vision_fusion.cpp
+++ b/range_vision_fusion/src/range_vision_fusion.cpp
@@ -22,695 +22,611 @@
 
 #include "range_vision_fusion/range_vision_fusion.h"
 
-cv::Point3f
-ROSRangeVisionFusionApp::TransformPoint(const geometry_msgs::Point &in_point, const tf::StampedTransform &in_transform)
-{
-  tf::Vector3 tf_point(in_point.x, in_point.y, in_point.z);
-  tf::Vector3 tf_point_t = in_transform * tf_point;
-  return cv::Point3f(tf_point_t.x(), tf_point_t.y(), tf_point_t.z());
+cv::Point3f ROSRangeVisionFusionApp::TransformPoint(
+    const geometry_msgs::Point &in_point,
+    const tf::StampedTransform &in_transform) {
+    tf::Vector3 tf_point(in_point.x, in_point.y, in_point.z);
+    tf::Vector3 tf_point_t = in_transform * tf_point;
+    return cv::Point3f(tf_point_t.x(), tf_point_t.y(), tf_point_t.z());
 }
 
-cv::Point2i
-ROSRangeVisionFusionApp::ProjectPoint(const cv::Point3f &in_point)
-{
-  auto u = int(in_point.x * fx_ / in_point.z + cx_);
-  auto v = int(in_point.y * fy_ / in_point.z + cy_);
+cv::Point2i ROSRangeVisionFusionApp::ProjectPoint(const cv::Point3f &in_point) {
+    auto u = int(in_point.x * fx_ / in_point.z + cx_);
+    auto v = int(in_point.y * fy_ / in_point.z + cy_);
 
-  return cv::Point2i(u, v);
+    return cv::Point2i(u, v);
 }
 
-autoware_msgs::DetectedObject
-ROSRangeVisionFusionApp::TransformObject(const autoware_msgs::DetectedObject &in_detection,
-                                         const tf::StampedTransform &in_transform)
-{
-  autoware_msgs::DetectedObject t_obj = in_detection;
-
-  tf::Vector3 in_pos(in_detection.pose.position.x,
-                     in_detection.pose.position.y,
-                     in_detection.pose.position.z);
-  tf::Quaternion in_quat(in_detection.pose.orientation.x,
-                         in_detection.pose.orientation.y,
-                         in_detection.pose.orientation.w,
-                         in_detection.pose.orientation.z);
-
-  tf::Vector3 in_pos_t = in_transform * in_pos;
-  tf::Quaternion in_quat_t = in_transform * in_quat;
-
-  t_obj.pose.position.x = in_pos_t.x();
-  t_obj.pose.position.y = in_pos_t.y();
-  t_obj.pose.position.z = in_pos_t.z();
-
-  t_obj.pose.orientation.x = in_quat_t.x();
-  t_obj.pose.orientation.y = in_quat_t.y();
-  t_obj.pose.orientation.z = in_quat_t.z();
-  t_obj.pose.orientation.w = in_quat_t.w();
-
-  return t_obj;
+autoware_msgs::DetectedObject ROSRangeVisionFusionApp::TransformObject(
+    const autoware_msgs::DetectedObject &in_detection,
+    const tf::StampedTransform &in_transform) {
+    autoware_msgs::DetectedObject t_obj = in_detection;
+
+    tf::Vector3 in_pos(in_detection.pose.position.x,
+                       in_detection.pose.position.y,
+                       in_detection.pose.position.z);
+    tf::Quaternion in_quat(
+        in_detection.pose.orientation.x, in_detection.pose.orientation.y,
+        in_detection.pose.orientation.w, in_detection.pose.orientation.z);
+
+    tf::Vector3 in_pos_t = in_transform * in_pos;
+    tf::Quaternion in_quat_t = in_transform * in_quat;
+
+    t_obj.pose.position.x = in_pos_t.x();
+    t_obj.pose.position.y = in_pos_t.y();
+    t_obj.pose.position.z = in_pos_t.z();
+
+    t_obj.pose.orientation.x = in_quat_t.x();
+    t_obj.pose.orientation.y = in_quat_t.y();
+    t_obj.pose.orientation.z = in_quat_t.z();
+    t_obj.pose.orientation.w = in_quat_t.w();
+
+    return t_obj;
 }
 
-bool
-ROSRangeVisionFusionApp::IsObjectInImage(const autoware_msgs::DetectedObject &in_detection)
-{
-  cv::Point3f image_space_point = TransformPoint(in_detection.pose.position, camera_lidar_tf_);
+bool ROSRangeVisionFusionApp::IsObjectInImage(
+    const autoware_msgs::DetectedObject &in_detection) {
+    cv::Point3f image_space_point =
+        TransformPoint(in_detection.pose.position, camera_lidar_tf_);
 
-  cv::Point2i image_pixel = ProjectPoint(image_space_point);
+    cv::Point2i image_pixel = ProjectPoint(image_space_point);
 
-  return (image_pixel.x >= 0)
-         && (image_pixel.x < image_size_.width)
-         && (image_pixel.y >= 0)
-         && (image_pixel.y < image_size_.height)
-         && (image_space_point.z > 0);
+    return (image_pixel.x >= 0) && (image_pixel.x < image_size_.width) &&
+           (image_pixel.y >= 0) && (image_pixel.y < image_size_.height) &&
+           (image_space_point.z > 0);
 }
 
-cv::Rect ROSRangeVisionFusionApp::ProjectDetectionToRect(const autoware_msgs::DetectedObject &in_detection)
-{
-  cv::Rect projected_box;
+cv::Rect ROSRangeVisionFusionApp::ProjectDetectionToRect(
+    const autoware_msgs::DetectedObject &in_detection) {
+    cv::Rect projected_box;
 
-  Eigen::Vector3f pos;
-  pos << in_detection.pose.position.x,
-    in_detection.pose.position.y,
-    in_detection.pose.position.z;
+    Eigen::Vector3f pos;
+    pos << in_detection.pose.position.x, in_detection.pose.position.y,
+        in_detection.pose.position.z;
 
-  Eigen::Quaternionf rot(in_detection.pose.orientation.w,
-                         in_detection.pose.orientation.x,
-                         in_detection.pose.orientation.y,
-                         in_detection.pose.orientation.z);
+    Eigen::Quaternionf rot(
+        in_detection.pose.orientation.w, in_detection.pose.orientation.x,
+        in_detection.pose.orientation.y, in_detection.pose.orientation.z);
 
-  std::vector<double> dims = {
-    in_detection.dimensions.x,
-    in_detection.dimensions.y,
-    in_detection.dimensions.z
-  };
+    std::vector<double> dims = {in_detection.dimensions.x,
+                                in_detection.dimensions.y,
+                                in_detection.dimensions.z};
 
-  jsk_recognition_utils::Cube cube(pos, rot, dims);
+    jsk_recognition_utils::Cube cube(pos, rot, dims);
 
-  Eigen::Affine3f range_vision_tf;
-  tf::transformTFToEigen(camera_lidar_tf_, range_vision_tf);
-  jsk_recognition_utils::Vertices vertices = cube.transformVertices(range_vision_tf);
+    Eigen::Affine3f range_vision_tf;
+    tf::transformTFToEigen(camera_lidar_tf_, range_vision_tf);
+    jsk_recognition_utils::Vertices vertices =
+        cube.transformVertices(range_vision_tf);
 
-  std::vector<cv::Point> polygon;
-  for (auto &vertex : vertices)
-  {
-    cv::Point p = ProjectPoint(cv::Point3f(vertex.x(), vertex.y(), vertex.z()));
-    polygon.push_back(p);
-  }
+    std::vector<cv::Point> polygon;
+    for (auto &vertex : vertices) {
+        cv::Point p =
+            ProjectPoint(cv::Point3f(vertex.x(), vertex.y(), vertex.z()));
+        polygon.push_back(p);
+    }
 
-  projected_box = cv::boundingRect(polygon);
+    projected_box = cv::boundingRect(polygon);
 
-  return projected_box;
+    return projected_box;
 }
 
-void
-ROSRangeVisionFusionApp::TransformRangeToVision(const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections,
-                                                autoware_msgs::DetectedObjectArray &out_in_cv_range_detections,
-                                                autoware_msgs::DetectedObjectArray &out_out_cv_range_detections)
-{
-  out_in_cv_range_detections.header = in_range_detections->header;
-  out_in_cv_range_detections.objects.clear();
-  out_out_cv_range_detections.header = in_range_detections->header;
-  out_out_cv_range_detections.objects.clear();
-  for (size_t i = 0; i < in_range_detections->objects.size(); i++)
-  {
-    if (IsObjectInImage(in_range_detections->objects[i]))
-    {
-      out_in_cv_range_detections.objects.push_back(in_range_detections->objects[i]);
-    } else
-    {
-      out_out_cv_range_detections.objects.push_back(in_range_detections->objects[i]);
+void ROSRangeVisionFusionApp::TransformRangeToVision(
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections,
+    autoware_msgs::DetectedObjectArray &out_in_cv_range_detections,
+    autoware_msgs::DetectedObjectArray &out_out_cv_range_detections) {
+    out_in_cv_range_detections.header = in_range_detections->header;
+    out_in_cv_range_detections.objects.clear();
+    out_out_cv_range_detections.header = in_range_detections->header;
+    out_out_cv_range_detections.objects.clear();
+    for (size_t i = 0; i < in_range_detections->objects.size(); i++) {
+        if (IsObjectInImage(in_range_detections->objects[i])) {
+            out_in_cv_range_detections.objects.push_back(
+                in_range_detections->objects[i]);
+        } else {
+            out_out_cv_range_detections.objects.push_back(
+                in_range_detections->objects[i]);
+        }
     }
-  }
 }
 
-void
-ROSRangeVisionFusionApp::CalculateObjectFeatures(autoware_msgs::DetectedObject &in_out_object, bool in_estimate_pose)
-{
-
-  float min_x = std::numeric_limits<float>::max();
-  float max_x = -std::numeric_limits<float>::max();
-  float min_y = std::numeric_limits<float>::max();
-  float max_y = -std::numeric_limits<float>::max();
-  float min_z = std::numeric_limits<float>::max();
-  float max_z = -std::numeric_limits<float>::max();
-  float average_x = 0, average_y = 0, average_z = 0, length, width, height;
-  pcl::PointXYZ centroid, min_point, max_point, average_point;
-
-  std::vector<cv::Point2f> object_2d_points;
-
-  pcl::PointCloud<pcl::PointXYZ> in_cloud;
-  pcl::fromROSMsg(in_out_object.pointcloud, in_cloud);
-
-  for (const auto &point : in_cloud.points)
-  {
-    average_x += point.x;
-    average_y += point.y;
-    average_z += point.z;
-    centroid.x += point.x;
-    centroid.y += point.y;
-    centroid.z += point.z;
-
-    if (point.x < min_x)
-      min_x = point.x;
-    if (point.y < min_y)
-      min_y = point.y;
-    if (point.z < min_z)
-      min_z = point.z;
-    if (point.x > max_x)
-      max_x = point.x;
-    if (point.y > max_y)
-      max_y = point.y;
-    if (point.z > max_z)
-      max_z = point.z;
-
-    cv::Point2f pt;
-    pt.x = point.x;
-    pt.y = point.y;
-    object_2d_points.push_back(pt);
-  }
-  min_point.x = min_x;
-  min_point.y = min_y;
-  min_point.z = min_z;
-  max_point.x = max_x;
-  max_point.y = max_y;
-  max_point.z = max_z;
-
-  if (in_cloud.points.size() > 0)
-  {
-    centroid.x /= in_cloud.points.size();
-    centroid.y /= in_cloud.points.size();
-    centroid.z /= in_cloud.points.size();
-
-    average_x /= in_cloud.points.size();
-    average_y /= in_cloud.points.size();
-    average_z /= in_cloud.points.size();
-  }
-
-  average_point.x = average_x;
-  average_point.y = average_y;
-  average_point.z = average_z;
-
-  length = max_point.x - min_point.x;
-  width = max_point.y - min_point.y;
-  height = max_point.z - min_point.z;
-
-  geometry_msgs::PolygonStamped convex_hull;
-  std::vector<cv::Point2f> hull_points;
-  if (object_2d_points.size() > 0)
-    cv::convexHull(object_2d_points, hull_points);
-
-  convex_hull.header = in_out_object.header;
-  for (size_t i = 0; i < hull_points.size() + 1; i++)
-  {
-    geometry_msgs::Point32 point;
-    point.x = hull_points[i % hull_points.size()].x;
-    point.y = hull_points[i % hull_points.size()].y;
-    point.z = min_point.z;
-    convex_hull.polygon.points.push_back(point);
-  }
-
-  for (size_t i = 0; i < hull_points.size() + 1; i++)
-  {
-    geometry_msgs::Point32 point;
-    point.x = hull_points[i % hull_points.size()].x;
-    point.y = hull_points[i % hull_points.size()].y;
-    point.z = max_point.z;
-    convex_hull.polygon.points.push_back(point);
-  }
-
-  double rz = 0;
-  if (in_estimate_pose)
-  {
-    cv::RotatedRect box = cv::minAreaRect(hull_points);
-    rz = box.angle * 3.14 / 180;
-    in_out_object.pose.position.x = box.center.x;
-    in_out_object.pose.position.y = box.center.y;
-    in_out_object.dimensions.x = box.size.width;
-    in_out_object.dimensions.y = box.size.height;
-  }
-
-  in_out_object.convex_hull = convex_hull;
-
-  in_out_object.pose.position.x = min_point.x + length / 2;
-  in_out_object.pose.position.y = min_point.y + width / 2;
-  in_out_object.pose.position.z = min_point.z + height / 2;
-
-  in_out_object.dimensions.x = ((length < 0) ? -1 * length : length);
-  in_out_object.dimensions.y = ((width < 0) ? -1 * width : width);
-  in_out_object.dimensions.z = ((height < 0) ? -1 * height : height);
-
-  tf::Quaternion quat = tf::createQuaternionFromRPY(0.0, 0.0, rz);
-  tf::quaternionTFToMsg(quat, in_out_object.pose.orientation);
-}
+void ROSRangeVisionFusionApp::CalculateObjectFeatures(
+    autoware_msgs::DetectedObject &in_out_object, bool in_estimate_pose) {
+    float min_x = std::numeric_limits<float>::max();
+    float max_x = -std::numeric_limits<float>::max();
+    float min_y = std::numeric_limits<float>::max();
+    float max_y = -std::numeric_limits<float>::max();
+    float min_z = std::numeric_limits<float>::max();
+    float max_z = -std::numeric_limits<float>::max();
+    float average_x = 0, average_y = 0, average_z = 0, length, width, height;
+    pcl::PointXYZ centroid, min_point, max_point, average_point;
+
+    std::vector<cv::Point2f> object_2d_points;
+
+    pcl::PointCloud<pcl::PointXYZ> in_cloud;
+    pcl::fromROSMsg(in_out_object.pointcloud, in_cloud);
+
+    for (const auto &point : in_cloud.points) {
+        average_x += point.x;
+        average_y += point.y;
+        average_z += point.z;
+        centroid.x += point.x;
+        centroid.y += point.y;
+        centroid.z += point.z;
+
+        if (point.x < min_x) min_x = point.x;
+        if (point.y < min_y) min_y = point.y;
+        if (point.z < min_z) min_z = point.z;
+        if (point.x > max_x) max_x = point.x;
+        if (point.y > max_y) max_y = point.y;
+        if (point.z > max_z) max_z = point.z;
+
+        cv::Point2f pt;
+        pt.x = point.x;
+        pt.y = point.y;
+        object_2d_points.push_back(pt);
+    }
+    min_point.x = min_x;
+    min_point.y = min_y;
+    min_point.z = min_z;
+    max_point.x = max_x;
+    max_point.y = max_y;
+    max_point.z = max_z;
+
+    if (in_cloud.points.size() > 0) {
+        centroid.x /= in_cloud.points.size();
+        centroid.y /= in_cloud.points.size();
+        centroid.z /= in_cloud.points.size();
+
+        average_x /= in_cloud.points.size();
+        average_y /= in_cloud.points.size();
+        average_z /= in_cloud.points.size();
+    }
 
-autoware_msgs::DetectedObject ROSRangeVisionFusionApp::MergeObjects(const autoware_msgs::DetectedObject &in_object_a,
-                                                                    const autoware_msgs::DetectedObject &in_object_b)
-{
-  autoware_msgs::DetectedObject object_merged;
-  object_merged = in_object_b;
+    average_point.x = average_x;
+    average_point.y = average_y;
+    average_point.z = average_z;
+
+    length = max_point.x - min_point.x;
+    width = max_point.y - min_point.y;
+    height = max_point.z - min_point.z;
+
+    geometry_msgs::PolygonStamped convex_hull;
+    std::vector<cv::Point2f> hull_points;
+    if (object_2d_points.size() > 0)
+        cv::convexHull(object_2d_points, hull_points);
+
+    convex_hull.header = in_out_object.header;
+    for (size_t i = 0; i < hull_points.size() + 1; i++) {
+        geometry_msgs::Point32 point;
+        point.x = hull_points[i % hull_points.size()].x;
+        point.y = hull_points[i % hull_points.size()].y;
+        point.z = min_point.z;
+        convex_hull.polygon.points.push_back(point);
+    }
 
-  pcl::PointCloud<pcl::PointXYZ> cloud_a, cloud_b, cloud_merged;
+    for (size_t i = 0; i < hull_points.size() + 1; i++) {
+        geometry_msgs::Point32 point;
+        point.x = hull_points[i % hull_points.size()].x;
+        point.y = hull_points[i % hull_points.size()].y;
+        point.z = max_point.z;
+        convex_hull.polygon.points.push_back(point);
+    }
 
-  if (!in_object_a.pointcloud.data.empty())
-    pcl::fromROSMsg(in_object_a.pointcloud, cloud_a);
-  if (!in_object_b.pointcloud.data.empty())
-    pcl::fromROSMsg(in_object_b.pointcloud, cloud_b);
+    double rz = 0;
+    if (in_estimate_pose) {
+        cv::RotatedRect box = cv::minAreaRect(hull_points);
+        rz = box.angle * 3.14 / 180;
+        in_out_object.pose.position.x = box.center.x;
+        in_out_object.pose.position.y = box.center.y;
+        in_out_object.dimensions.x = box.size.width;
+        in_out_object.dimensions.y = box.size.height;
+    }
+
+    in_out_object.convex_hull = convex_hull;
 
-  cloud_merged = cloud_a + cloud_b;
+    in_out_object.pose.position.x = min_point.x + length / 2;
+    in_out_object.pose.position.y = min_point.y + width / 2;
+    in_out_object.pose.position.z = min_point.z + height / 2;
 
-  sensor_msgs::PointCloud2 cloud_msg;
-  pcl::toROSMsg(cloud_merged, cloud_msg);
-  cloud_msg.header = object_merged.pointcloud.header;
+    in_out_object.dimensions.x = ((length < 0) ? -1 * length : length);
+    in_out_object.dimensions.y = ((width < 0) ? -1 * width : width);
+    in_out_object.dimensions.z = ((height < 0) ? -1 * height : height);
 
-  object_merged.pointcloud = cloud_msg;
+    tf::Quaternion quat = tf::createQuaternionFromRPY(0.0, 0.0, rz);
+    tf::quaternionTFToMsg(quat, in_out_object.pose.orientation);
+}
 
-  return object_merged;
+autoware_msgs::DetectedObject ROSRangeVisionFusionApp::MergeObjects(
+    const autoware_msgs::DetectedObject &in_object_a,
+    const autoware_msgs::DetectedObject &in_object_b) {
+    autoware_msgs::DetectedObject object_merged;
+    object_merged = in_object_b;
 
+    pcl::PointCloud<pcl::PointXYZ> cloud_a, cloud_b, cloud_merged;
+
+    if (!in_object_a.pointcloud.data.empty())
+        pcl::fromROSMsg(in_object_a.pointcloud, cloud_a);
+    if (!in_object_b.pointcloud.data.empty())
+        pcl::fromROSMsg(in_object_b.pointcloud, cloud_b);
+
+    cloud_merged = cloud_a + cloud_b;
+
+    sensor_msgs::PointCloud2 cloud_msg;
+    pcl::toROSMsg(cloud_merged, cloud_msg);
+    cloud_msg.header = object_merged.pointcloud.header;
+
+    object_merged.pointcloud = cloud_msg;
+
+    return object_merged;
 }
 
-double ROSRangeVisionFusionApp::GetDistanceToObject(const autoware_msgs::DetectedObject &in_object)
-{
-  return sqrt(in_object.dimensions.x * in_object.dimensions.x +
-              in_object.dimensions.y * in_object.dimensions.y +
-              in_object.dimensions.z * in_object.dimensions.z);
+double ROSRangeVisionFusionApp::GetDistanceToObject(
+    const autoware_msgs::DetectedObject &in_object) {
+    return sqrt(in_object.dimensions.x * in_object.dimensions.x +
+                in_object.dimensions.y * in_object.dimensions.y +
+                in_object.dimensions.z * in_object.dimensions.z);
 }
 
-void ROSRangeVisionFusionApp::CheckMinimumDimensions(autoware_msgs::DetectedObject &in_out_object)
-{
-  if (in_out_object.label == "car")
-  {
-    if (in_out_object.dimensions.x < car_depth_)
-      in_out_object.dimensions.x = car_depth_;
-    if (in_out_object.dimensions.y < car_width_)
-      in_out_object.dimensions.y = car_width_;
-    if (in_out_object.dimensions.z < car_height_)
-      in_out_object.dimensions.z = car_height_;
-  }
-  if (in_out_object.label == "person")
-  {
-    if (in_out_object.dimensions.x < person_depth_)
-      in_out_object.dimensions.x = person_depth_;
-    if (in_out_object.dimensions.y < person_width_)
-      in_out_object.dimensions.y = person_width_;
-    if (in_out_object.dimensions.z < person_height_)
-      in_out_object.dimensions.z = person_height_;
-  }
-
-  if (in_out_object.label == "truck" || in_out_object.label == "bus")
-  {
-    if (in_out_object.dimensions.x < truck_depth_)
-      in_out_object.dimensions.x = truck_depth_;
-    if (in_out_object.dimensions.y < truck_width_)
-      in_out_object.dimensions.y = truck_width_;
-    if (in_out_object.dimensions.z < truck_height_)
-      in_out_object.dimensions.z = truck_height_;
-  }
+void ROSRangeVisionFusionApp::CheckMinimumDimensions(
+    autoware_msgs::DetectedObject &in_out_object) {
+    if (in_out_object.label == "car") {
+        if (in_out_object.dimensions.x < car_depth_)
+            in_out_object.dimensions.x = car_depth_;
+        if (in_out_object.dimensions.y < car_width_)
+            in_out_object.dimensions.y = car_width_;
+        if (in_out_object.dimensions.z < car_height_)
+            in_out_object.dimensions.z = car_height_;
+    }
+    if (in_out_object.label == "person") {
+        if (in_out_object.dimensions.x < person_depth_)
+            in_out_object.dimensions.x = person_depth_;
+        if (in_out_object.dimensions.y < person_width_)
+            in_out_object.dimensions.y = person_width_;
+        if (in_out_object.dimensions.z < person_height_)
+            in_out_object.dimensions.z = person_height_;
+    }
+
+    if (in_out_object.label == "truck" || in_out_object.label == "bus") {
+        if (in_out_object.dimensions.x < truck_depth_)
+            in_out_object.dimensions.x = truck_depth_;
+        if (in_out_object.dimensions.y < truck_width_)
+            in_out_object.dimensions.y = truck_width_;
+        if (in_out_object.dimensions.z < truck_height_)
+            in_out_object.dimensions.z = truck_height_;
+    }
 }
 
 autoware_msgs::DetectedObjectArray
 ROSRangeVisionFusionApp::FuseRangeVisionDetections(
-  const autoware_msgs::DetectedObjectArray::ConstPtr &in_vision_detections,
-  const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections)
-{
-
-  autoware_msgs::DetectedObjectArray range_in_cv;
-  autoware_msgs::DetectedObjectArray range_out_cv;
-  TransformRangeToVision(in_range_detections, range_in_cv, range_out_cv);
-
-  autoware_msgs::DetectedObjectArray fused_objects;
-  fused_objects.header = in_range_detections->header;
-
-  std::vector<std::vector<size_t> > vision_range_assignments(in_vision_detections->objects.size());
-  std::vector<bool> used_vision_detections(in_vision_detections->objects.size(), false);
-  std::vector<long> vision_range_closest(in_vision_detections->objects.size());
-
-  for (size_t i = 0; i < in_vision_detections->objects.size(); i++)
-  {
-    auto vision_object = in_vision_detections->objects[i];
-
-    cv::Rect vision_rect(vision_object.x, vision_object.y,
-                         vision_object.width, vision_object.height);
-    int vision_rect_area = vision_rect.area();
-    long closest_index = -1;
-    double closest_distance = std::numeric_limits<double>::max();
-
-    for (size_t j = 0; j < range_in_cv.objects.size(); j++)
-    {
-      double current_distance = GetDistanceToObject(range_in_cv.objects[j]);
-
-      cv::Rect range_rect = ProjectDetectionToRect(range_in_cv.objects[j]);
-      int range_rect_area = range_rect.area();
-
-      cv::Rect overlap = range_rect & vision_rect;
-      if ((overlap.area() > range_rect_area * overlap_threshold_)
-          || (overlap.area() > vision_rect_area * overlap_threshold_)
-        )
-      {
-        vision_range_assignments[i].push_back(j);
-        range_in_cv.objects[j].score = vision_object.score;
-        range_in_cv.objects[j].label = vision_object.label;
-        range_in_cv.objects[j].color = vision_object.color;
-        range_in_cv.objects[j].image_frame = vision_object.image_frame;
-        range_in_cv.objects[j].x = vision_object.x;
-        range_in_cv.objects[j].y = vision_object.y;
-        range_in_cv.objects[j].width = vision_object.width;
-        range_in_cv.objects[j].height = vision_object.height;
-        range_in_cv.objects[j].angle = vision_object.angle;
-        range_in_cv.objects[j].id = vision_object.id;
-        CheckMinimumDimensions(range_in_cv.objects[j]);
-        if (vision_object.pose.orientation.x > 0
-            || vision_object.pose.orientation.y > 0
-            || vision_object.pose.orientation.z > 0)
-        {
-          range_in_cv.objects[i].pose.orientation = vision_object.pose.orientation;
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_vision_detections,
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections) {
+    autoware_msgs::DetectedObjectArray range_in_cv;
+    autoware_msgs::DetectedObjectArray range_out_cv;
+    TransformRangeToVision(in_range_detections, range_in_cv, range_out_cv);
+
+    autoware_msgs::DetectedObjectArray fused_objects;
+    fused_objects.header = in_range_detections->header;
+
+    std::vector<std::vector<size_t> > vision_range_assignments(
+        in_vision_detections->objects.size());
+    std::vector<bool> used_vision_detections(
+        in_vision_detections->objects.size(), false);
+    std::vector<long> vision_range_closest(
+        in_vision_detections->objects.size());
+
+    for (size_t i = 0; i < in_vision_detections->objects.size(); i++) {
+        auto vision_object = in_vision_detections->objects[i];
+
+        cv::Rect vision_rect(vision_object.x, vision_object.y,
+                             vision_object.width, vision_object.height);
+        int vision_rect_area = vision_rect.area();
+        long closest_index = -1;
+        double closest_distance = std::numeric_limits<double>::max();
+
+        for (size_t j = 0; j < range_in_cv.objects.size(); j++) {
+            double current_distance =
+                GetDistanceToObject(range_in_cv.objects[j]);
+
+            cv::Rect range_rect =
+                ProjectDetectionToRect(range_in_cv.objects[j]);
+            int range_rect_area = range_rect.area();
+
+            cv::Rect overlap = range_rect & vision_rect;
+            if ((overlap.area() > range_rect_area * overlap_threshold_) ||
+                (overlap.area() > vision_rect_area * overlap_threshold_)) {
+                vision_range_assignments[i].push_back(j);
+                range_in_cv.objects[j].score = vision_object.score;
+                range_in_cv.objects[j].label = vision_object.label;
+                range_in_cv.objects[j].color = vision_object.color;
+                range_in_cv.objects[j].image_frame = vision_object.image_frame;
+                range_in_cv.objects[j].x = vision_object.x;
+                range_in_cv.objects[j].y = vision_object.y;
+                range_in_cv.objects[j].width = vision_object.width;
+                range_in_cv.objects[j].height = vision_object.height;
+                range_in_cv.objects[j].angle = vision_object.angle;
+                range_in_cv.objects[j].id = vision_object.id;
+                CheckMinimumDimensions(range_in_cv.objects[j]);
+                if (vision_object.pose.orientation.x > 0 ||
+                    vision_object.pose.orientation.y > 0 ||
+                    vision_object.pose.orientation.z > 0) {
+                    range_in_cv.objects[i].pose.orientation =
+                        vision_object.pose.orientation;
+                }
+                if (current_distance < closest_distance) {
+                    closest_index = j;
+                    closest_distance = current_distance;
+                }
+                used_vision_detections[i] = true;
+            }  // end if overlap
+        }      // end for range_in_cv
+        vision_range_closest[i] = closest_index;
+    }
+
+    std::vector<bool> used_range_detections(range_in_cv.objects.size(), false);
+    // only assign the closest
+    for (size_t i = 0; i < vision_range_assignments.size(); i++) {
+        if (!range_in_cv.objects.empty() && vision_range_closest[i] >= 0) {
+            used_range_detections[i] = true;
+            fused_objects.objects.push_back(
+                range_in_cv.objects[vision_range_closest[i]]);
         }
-        if (current_distance < closest_distance)
-        {
-          closest_index = j;
-          closest_distance = current_distance;
+    }
+    for (size_t i = 0; i < used_vision_detections.size(); i++) {
+        if (!used_vision_detections[i]) {
+            fused_objects.objects.push_back(in_vision_detections->objects[i]);
         }
-        used_vision_detections[i] = true;
-      }//end if overlap
-    }//end for range_in_cv
-    vision_range_closest[i] = closest_index;
-  }
-
-  std::vector<bool> used_range_detections(range_in_cv.objects.size(), false);
-  //only assign the closest
-  for (size_t i = 0; i < vision_range_assignments.size(); i++)
-  {
-    if (!range_in_cv.objects.empty() && vision_range_closest[i] >= 0)
-    {
-      used_range_detections[i] = true;
-      fused_objects.objects.push_back(range_in_cv.objects[vision_range_closest[i]]);
     }
-  }
-  for (size_t i = 0; i < used_vision_detections.size(); i++)
-  {
-    if (!used_vision_detections[i])
-    {
-      fused_objects.objects.push_back(in_vision_detections->objects[i]);
+    // add also objects outside the image
+    for (auto &object : range_out_cv.objects) {
+        fused_objects.objects.push_back(object);
+    }
+    // enable merged for visualization
+    for (auto &object : fused_objects.objects) {
+        object.valid = true;
     }
-  }
-  //add also objects outside the image
-  for (auto &object: range_out_cv.objects)
-  {
-    fused_objects.objects.push_back(object);
-  }
-  //enable merged for visualization
-  for (auto &object : fused_objects.objects)
-  {
-    object.valid = true;
-  }
-
-  return fused_objects;
+
+    return fused_objects;
 }
 
-void
-ROSRangeVisionFusionApp::SyncedDetectionsCallback(
-  const autoware_msgs::DetectedObjectArray::ConstPtr &in_vision_detections,
-  const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections)
-{
-  autoware_msgs::DetectedObjectArray fusion_objects;
-  fusion_objects.objects.clear();
-
-  if (empty_frames_ > 5)
-  {
-    ROS_INFO("[%s] Empty Detections. Make sure the vision and range detectors are running.", __APP_NAME__);
-  }
-
-  if (nullptr == in_vision_detections
-      && nullptr == in_range_detections)
-  {
-    empty_frames_++;
-    return;
-  }
-
-  if (nullptr == in_vision_detections
-      && nullptr != in_range_detections
-      && !in_range_detections->objects.empty())
-  {
-    publisher_fused_objects_.publish(in_range_detections);
-    empty_frames_++;
-    return;
-  }
-  if (nullptr == in_range_detections
-      && nullptr != in_vision_detections
-      && !in_vision_detections->objects.empty())
-  {
-    publisher_fused_objects_.publish(in_vision_detections);
-    empty_frames_++;
-    return;
-  }
-
-  if (!camera_lidar_tf_ok_)
-  {
-    camera_lidar_tf_ = FindTransform(image_frame_id_,
-                                     in_range_detections->header.frame_id);
-  }
-  if (
-    !camera_lidar_tf_ok_ ||
-    !camera_info_ok_)
-  {
-    ROS_INFO("[%s] Missing Camera-LiDAR TF or CameraInfo", __APP_NAME__);
-    return;
-  }
-
-  fusion_objects = FuseRangeVisionDetections(in_vision_detections, in_range_detections);
-
-  publisher_fused_objects_.publish(fusion_objects);
-  empty_frames_ = 0;
-
-  vision_detections_ = nullptr;
-  range_detections_ = nullptr;
+void ROSRangeVisionFusionApp::SyncedDetectionsCallback(
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_vision_detections,
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections) {
+    autoware_msgs::DetectedObjectArray fusion_objects;
+    fusion_objects.objects.clear();
+
+    if (empty_frames_ > 5) {
+        ROS_INFO(
+            "[%s] Empty Detections. Make sure the vision and range detectors "
+            "are running.",
+            __APP_NAME__);
+    }
+
+    if (nullptr == in_vision_detections && nullptr == in_range_detections) {
+        empty_frames_++;
+        return;
+    }
+
+    if (nullptr == in_vision_detections && nullptr != in_range_detections &&
+        !in_range_detections->objects.empty()) {
+        publisher_fused_objects_.publish(in_range_detections);
+        empty_frames_++;
+        return;
+    }
+    if (nullptr == in_range_detections && nullptr != in_vision_detections &&
+        !in_vision_detections->objects.empty()) {
+        publisher_fused_objects_.publish(in_vision_detections);
+        empty_frames_++;
+        return;
+    }
 
+    if (!camera_lidar_tf_ok_) {
+        camera_lidar_tf_ = FindTransform(image_frame_id_,
+                                         in_range_detections->header.frame_id);
+    }
+    if (!camera_lidar_tf_ok_ || !camera_info_ok_) {
+        ROS_INFO("[%s] Missing Camera-LiDAR TF or CameraInfo", __APP_NAME__);
+        return;
+    }
+
+    fusion_objects =
+        FuseRangeVisionDetections(in_vision_detections, in_range_detections);
+
+    publisher_fused_objects_.publish(fusion_objects);
+    empty_frames_ = 0;
+
+    vision_detections_ = nullptr;
+    range_detections_ = nullptr;
 }
 
-void
-ROSRangeVisionFusionApp::VisionDetectionsCallback(
-  const autoware_msgs::DetectedObjectArray::ConstPtr &in_vision_detections)
-{
-  if (!processing_ && !in_vision_detections->objects.empty())
-  {
-    processing_ = true;
-    vision_detections_ = in_vision_detections;
-    SyncedDetectionsCallback(in_vision_detections, range_detections_);
-    processing_ = false;
-  }
+void ROSRangeVisionFusionApp::VisionDetectionsCallback(
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_vision_detections) {
+    if (!processing_ && !in_vision_detections->objects.empty()) {
+        processing_ = true;
+        vision_detections_ = in_vision_detections;
+        SyncedDetectionsCallback(in_vision_detections, range_detections_);
+        processing_ = false;
+    }
 }
 
-void
-ROSRangeVisionFusionApp::RangeDetectionsCallback(
-  const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections)
-{
-  if (!processing_ && !in_range_detections->objects.empty())
-  {
-    processing_ = true;
-    range_detections_ = in_range_detections;
-    SyncedDetectionsCallback(vision_detections_, in_range_detections);
-    processing_ = false;
-  }
+void ROSRangeVisionFusionApp::RangeDetectionsCallback(
+    const autoware_msgs::DetectedObjectArray::ConstPtr &in_range_detections) {
+    if (!processing_ && !in_range_detections->objects.empty()) {
+        processing_ = true;
+        range_detections_ = in_range_detections;
+        SyncedDetectionsCallback(vision_detections_, in_range_detections);
+        processing_ = false;
+    }
 }
 
-void ROSRangeVisionFusionApp::ImageCallback(const sensor_msgs::Image::ConstPtr &in_image_msg)
-{
-  if (!camera_info_ok_)
-    return;
-  cv_bridge::CvImagePtr cv_image = cv_bridge::toCvCopy(in_image_msg, "bgr8");
-  cv::Mat in_image = cv_image->image;
+void ROSRangeVisionFusionApp::ImageCallback(
+    const sensor_msgs::Image::ConstPtr &in_image_msg) {
+    if (!camera_info_ok_) return;
+    cv_bridge::CvImagePtr cv_image = cv_bridge::toCvCopy(in_image_msg, "bgr8");
+    cv::Mat in_image = cv_image->image;
 
-  cv::Mat undistorted_image;
-  cv::undistort(in_image, image_, camera_instrinsics_, distortion_coefficients_);
+    cv::Mat undistorted_image;
+    cv::undistort(in_image, image_, camera_instrinsics_,
+                  distortion_coefficients_);
 };
 
-void
-ROSRangeVisionFusionApp::IntrinsicsCallback(const sensor_msgs::CameraInfo &in_message)
-{
-  image_size_.height = in_message.height;
-  image_size_.width = in_message.width;
-
-  camera_instrinsics_ = cv::Mat(3, 3, CV_64F);
-  for (int row = 0; row < 3; row++)
-  {
-    for (int col = 0; col < 3; col++)
-    {
-      camera_instrinsics_.at<double>(row, col) = in_message.K[row * 3 + col];
+void ROSRangeVisionFusionApp::IntrinsicsCallback(
+    const sensor_msgs::CameraInfo &in_message) {
+    image_size_.height = in_message.height;
+    image_size_.width = in_message.width;
+
+    camera_instrinsics_ = cv::Mat(3, 3, CV_64F);
+    for (int row = 0; row < 3; row++) {
+        for (int col = 0; col < 3; col++) {
+            camera_instrinsics_.at<double>(row, col) =
+                in_message.K[row * 3 + col];
+        }
     }
-  }
-
-  distortion_coefficients_ = cv::Mat(1, 5, CV_64F);
-  for (int col = 0; col < 5; col++)
-  {
-    distortion_coefficients_.at<double>(col) = in_message.D[col];
-  }
-
-  fx_ = static_cast<float>(in_message.P[0]);
-  fy_ = static_cast<float>(in_message.P[5]);
-  cx_ = static_cast<float>(in_message.P[2]);
-  cy_ = static_cast<float>(in_message.P[6]);
-
-  intrinsics_subscriber_.shutdown();
-  camera_info_ok_ = true;
-  image_frame_id_ = in_message.header.frame_id;
-  ROS_INFO("[%s] CameraIntrinsics obtained.", __APP_NAME__);
-}
 
-tf::StampedTransform
-ROSRangeVisionFusionApp::FindTransform(const std::string &in_target_frame, const std::string &in_source_frame)
-{
-  tf::StampedTransform transform;
-
-  ROS_INFO("%s - > %s", in_source_frame.c_str(), in_target_frame.c_str());
-  camera_lidar_tf_ok_ = false;
-  try
-  {
-    transform_listener_->lookupTransform(in_target_frame, in_source_frame, ros::Time(0), transform);
-    camera_lidar_tf_ok_ = true;
-    ROS_INFO("[%s] Camera-Lidar TF obtained", __APP_NAME__);
-  }
-  catch (tf::TransformException &ex)
-  {
-    ROS_ERROR("[%s] %s", __APP_NAME__, ex.what());
-  }
-
-  return transform;
+    distortion_coefficients_ = cv::Mat(1, 5, CV_64F);
+    for (int col = 0; col < 5; col++) {
+        distortion_coefficients_.at<double>(col) = in_message.D[col];
+    }
+
+    fx_ = static_cast<float>(in_message.P[0]);
+    fy_ = static_cast<float>(in_message.P[5]);
+    cx_ = static_cast<float>(in_message.P[2]);
+    cy_ = static_cast<float>(in_message.P[6]);
+
+    intrinsics_subscriber_.shutdown();
+    camera_info_ok_ = true;
+    image_frame_id_ = in_message.header.frame_id;
+    ROS_INFO("[%s] CameraIntrinsics obtained.", __APP_NAME__);
 }
 
-void
-ROSRangeVisionFusionApp::InitializeROSIo(ros::NodeHandle &in_private_handle)
-{
-  //get params
-  std::string camera_info_src, detected_objects_vision, min_car_dimensions, min_person_dimensions, min_truck_dimensions;
-  std::string detected_objects_range, fused_topic_str = "/detection/fusion_tools/objects";
-  std::string name_space_str = ros::this_node::getNamespace();
-  bool sync_topics = false;
-
-  ROS_INFO(
-    "[%s] This node requires: Registered TF(Lidar-Camera), CameraInfo, Vision and Range Detections being published.",
-    __APP_NAME__);
-  in_private_handle.param<std::string>("detected_objects_range", detected_objects_range,
-                                       "/detection/lidar_detector/objects");
-  ROS_INFO("[%s] detected_objects_range: %s", __APP_NAME__, detected_objects_range.c_str());
-
-  in_private_handle.param<std::string>("detected_objects_vision", detected_objects_vision,
-                                       "/detection/image_detector/objects");
-  ROS_INFO("[%s] detected_objects_vision: %s", __APP_NAME__, detected_objects_vision.c_str());
-
-  in_private_handle.param<std::string>("camera_info_src", camera_info_src, "/camera_info");
-  ROS_INFO("[%s] camera_info_src: %s", __APP_NAME__, camera_info_src.c_str());
-
-  in_private_handle.param<double>("overlap_threshold", overlap_threshold_, 0.6);
-  ROS_INFO("[%s] overlap_threshold: %f", __APP_NAME__, overlap_threshold_);
-
-  in_private_handle.param<std::string>("min_car_dimensions", min_car_dimensions, "[3,2,2]");//w,h,d
-  ROS_INFO("[%s] min_car_dimensions: %s", __APP_NAME__, min_car_dimensions.c_str());
-
-  in_private_handle.param<std::string>("min_person_dimensions", min_person_dimensions, "[1,2,1]");
-  ROS_INFO("[%s] min_person_dimensions: %s", __APP_NAME__, min_person_dimensions.c_str());
-
-  in_private_handle.param<std::string>("min_truck_dimensions", min_truck_dimensions, "[4,2,2]");
-  ROS_INFO("[%s] min_truck_dimensions: %s", __APP_NAME__, min_truck_dimensions.c_str());
-
-
-  in_private_handle.param<bool>("sync_topics", sync_topics, false);
-  ROS_INFO("[%s] sync_topics: %d", __APP_NAME__, sync_topics);
-
-  YAML::Node car_dimensions = YAML::Load(min_car_dimensions);
-  YAML::Node person_dimensions = YAML::Load(min_person_dimensions);
-  YAML::Node truck_dimensions = YAML::Load(min_truck_dimensions);
-
-  if (car_dimensions.size() == 3)
-  {
-    car_width_ = car_dimensions[0].as<double>();
-    car_height_ = car_dimensions[1].as<double>();
-    car_depth_ = car_dimensions[2].as<double>();
-  }
-  if (person_dimensions.size() == 3)
-  {
-    person_width_ = person_dimensions[0].as<double>();
-    person_height_ = person_dimensions[1].as<double>();
-    person_depth_ = person_dimensions[2].as<double>();
-  }
-  if (truck_dimensions.size() == 3)
-  {
-    truck_width_ = truck_dimensions[0].as<double>();
-    truck_height_ = truck_dimensions[1].as<double>();
-    truck_depth_ = truck_dimensions[2].as<double>();
-  }
-
-  if (name_space_str != "/")
-  {
-    if (name_space_str.substr(0, 2) == "//")
-    {
-      name_space_str.erase(name_space_str.begin());
+tf::StampedTransform ROSRangeVisionFusionApp::FindTransform(
+    const std::string &in_target_frame, const std::string &in_source_frame) {
+    tf::StampedTransform transform;
+
+    ROS_INFO("%s - > %s", in_source_frame.c_str(), in_target_frame.c_str());
+    camera_lidar_tf_ok_ = false;
+    try {
+        transform_listener_->lookupTransform(in_target_frame, in_source_frame,
+                                             ros::Time(0), transform);
+        camera_lidar_tf_ok_ = true;
+        ROS_INFO("[%s] Camera-Lidar TF obtained", __APP_NAME__);
+    } catch (tf::TransformException &ex) {
+        ROS_ERROR("[%s] %s", __APP_NAME__, ex.what());
     }
-    camera_info_src = name_space_str + camera_info_src;
-  }
-
-  //generate subscribers and sychronizers
-  ROS_INFO("[%s] Subscribing to... %s", __APP_NAME__, camera_info_src.c_str());
-  intrinsics_subscriber_ = in_private_handle.subscribe(camera_info_src,
-                                                       1,
-                                                       &ROSRangeVisionFusionApp::IntrinsicsCallback, this);
-
-  ROS_INFO("[%s] Subscribing to... %s", __APP_NAME__, detected_objects_vision.c_str());
-  ROS_INFO("[%s] Subscribing to... %s", __APP_NAME__, detected_objects_range.c_str());
-  if (!sync_topics)
-  {
-    detections_range_subscriber_ = in_private_handle.subscribe(detected_objects_vision,
-                                                               1,
-                                                               &ROSRangeVisionFusionApp::VisionDetectionsCallback,
-                                                               this);
-
-    detections_vision_subscriber_ = in_private_handle.subscribe(detected_objects_range,
-                                                                1,
-                                                                &ROSRangeVisionFusionApp::RangeDetectionsCallback,
-                                                                this);
-  }
-  else
-  {
-    vision_filter_subscriber_ = new message_filters::Subscriber<autoware_msgs::DetectedObjectArray>(node_handle_,
-                                                                                                    detected_objects_vision,
-                                                                                                    1);
-    range_filter_subscriber_ = new message_filters::Subscriber<autoware_msgs::DetectedObjectArray>(node_handle_,
-                                                                                                   detected_objects_range,
-                                                                                                   1);
-    detections_synchronizer_ =
-      new message_filters::Synchronizer<SyncPolicyT>(SyncPolicyT(10),
-                                                     *vision_filter_subscriber_,
-                                                     *range_filter_subscriber_);
-    detections_synchronizer_->registerCallback(
-      boost::bind(&ROSRangeVisionFusionApp::SyncedDetectionsCallback, this, _1, _2));
-  }
-
-  publisher_fused_objects_ = node_handle_.advertise<autoware_msgs::DetectedObjectArray>(fused_topic_str, 1);
-
-  ROS_INFO("[%s] Publishing fused objects in %s", __APP_NAME__, fused_topic_str.c_str());
 
+    return transform;
 }
 
+void ROSRangeVisionFusionApp::InitializeROSIo(
+    ros::NodeHandle &in_private_handle) {
+    std::string detected_objects_range = "/input_range_objects";
+    std::string detected_objects_vision = "/input_vision_objects";
+    std::string camera_info_src = "/input_camera_info";
+    std::string fused_topic_str = "/output_fused_objects";
+
+    // get params
+    std::string min_car_dimensions, min_person_dimensions, min_truck_dimensions;
+    bool sync_topics = false;
+
+    ROS_INFO(
+        "[%s] This node requires: Registered TF(Lidar-Camera), CameraInfo, "
+        "Vision and Range Detections being published.",
+        __APP_NAME__);
+
+    in_private_handle.param<double>("overlap_threshold", overlap_threshold_,
+                                    0.6);
+    ROS_INFO("[%s] overlap_threshold: %f", __APP_NAME__, overlap_threshold_);
+
+    in_private_handle.param<std::string>(
+        "min_car_dimensions", min_car_dimensions, "[3,2,2]");  // w,h,d
+    ROS_INFO("[%s] min_car_dimensions: %s", __APP_NAME__,
+             min_car_dimensions.c_str());
+
+    in_private_handle.param<std::string>("min_person_dimensions",
+                                         min_person_dimensions, "[1,2,1]");
+    ROS_INFO("[%s] min_person_dimensions: %s", __APP_NAME__,
+             min_person_dimensions.c_str());
+
+    in_private_handle.param<std::string>("min_truck_dimensions",
+                                         min_truck_dimensions, "[4,2,2]");
+    ROS_INFO("[%s] min_truck_dimensions: %s", __APP_NAME__,
+             min_truck_dimensions.c_str());
+
+    in_private_handle.param<bool>("sync_topics", sync_topics, false);
+    ROS_INFO("[%s] sync_topics: %d", __APP_NAME__, sync_topics);
+
+    YAML::Node car_dimensions = YAML::Load(min_car_dimensions);
+    YAML::Node person_dimensions = YAML::Load(min_person_dimensions);
+    YAML::Node truck_dimensions = YAML::Load(min_truck_dimensions);
+
+    if (car_dimensions.size() == 3) {
+        car_width_ = car_dimensions[0].as<double>();
+        car_height_ = car_dimensions[1].as<double>();
+        car_depth_ = car_dimensions[2].as<double>();
+    }
+    if (person_dimensions.size() == 3) {
+        person_width_ = person_dimensions[0].as<double>();
+        person_height_ = person_dimensions[1].as<double>();
+        person_depth_ = person_dimensions[2].as<double>();
+    }
+    if (truck_dimensions.size() == 3) {
+        truck_width_ = truck_dimensions[0].as<double>();
+        truck_height_ = truck_dimensions[1].as<double>();
+        truck_depth_ = truck_dimensions[2].as<double>();
+    }
+
+    // generate subscribers and sychronizers
+    intrinsics_subscriber_ = in_private_handle.subscribe(
+        camera_info_src, 1, &ROSRangeVisionFusionApp::IntrinsicsCallback, this);
+
+    if (!sync_topics) {
+        detections_range_subscriber_ = in_private_handle.subscribe(
+            detected_objects_vision, 1,
+            &ROSRangeVisionFusionApp::VisionDetectionsCallback, this);
+
+        detections_vision_subscriber_ = in_private_handle.subscribe(
+            detected_objects_range, 1,
+            &ROSRangeVisionFusionApp::RangeDetectionsCallback, this);
+    } else {
+        vision_filter_subscriber_ =
+            new message_filters::Subscriber<autoware_msgs::DetectedObjectArray>(
+                node_handle_, detected_objects_vision, 1);
+        range_filter_subscriber_ =
+            new message_filters::Subscriber<autoware_msgs::DetectedObjectArray>(
+                node_handle_, detected_objects_range, 1);
+        detections_synchronizer_ =
+            new message_filters::Synchronizer<SyncPolicyT>(
+                SyncPolicyT(10), *vision_filter_subscriber_,
+                *range_filter_subscriber_);
+        detections_synchronizer_->registerCallback(boost::bind(
+            &ROSRangeVisionFusionApp::SyncedDetectionsCallback, this, _1, _2));
+    }
 
-void
-ROSRangeVisionFusionApp::Run()
-{
-  ros::NodeHandle private_node_handle("~");
-  tf::TransformListener transform_listener;
+    publisher_fused_objects_ =
+        node_handle_.advertise<autoware_msgs::DetectedObjectArray>(
+            fused_topic_str, 1);
+}
+
+void ROSRangeVisionFusionApp::Run() {
+    ros::NodeHandle private_node_handle("~");
+    tf::TransformListener transform_listener;
 
-  transform_listener_ = &transform_listener;
+    transform_listener_ = &transform_listener;
 
-  InitializeROSIo(private_node_handle);
+    InitializeROSIo(private_node_handle);
 
-  ROS_INFO("[%s] Ready. Waiting for data...", __APP_NAME__);
+    ROS_INFO("[%s] Ready. Waiting for data...", __APP_NAME__);
 
-  ros::spin();
+    ros::spin();
 
-  ROS_INFO("[%s] END", __APP_NAME__);
+    ROS_INFO("[%s] END", __APP_NAME__);
 }
 
-ROSRangeVisionFusionApp::ROSRangeVisionFusionApp()
-{
-  camera_lidar_tf_ok_ = false;
-  camera_info_ok_ = false;
-  processing_ = false;
-  image_frame_id_ = "";
-  overlap_threshold_ = 0.5;
-  empty_frames_ = 0;
+ROSRangeVisionFusionApp::ROSRangeVisionFusionApp() {
+    camera_lidar_tf_ok_ = false;
+    camera_info_ok_ = false;
+    processing_ = false;
+    image_frame_id_ = "";
+    overlap_threshold_ = 0.5;
+    empty_frames_ = 0;
 }
\ No newline at end of file
diff --git a/vision_darknet_detect/.gitignore b/vision_darknet_detect/.gitignore
index 0d1adb3..6bc4018 100644
--- a/vision_darknet_detect/.gitignore
+++ b/vision_darknet_detect/.gitignore
@@ -1 +1,2 @@
+!darknet/data/download_weights.sh
 darknet/data/*
\ No newline at end of file
diff --git a/vision_darknet_detect/interface.yaml b/vision_darknet_detect/interface.yaml
index 7a1b3c4..370cf69 100644
--- a/vision_darknet_detect/interface.yaml
+++ b/vision_darknet_detect/interface.yaml
@@ -1,3 +1,3 @@
 - name: vision_darknet_detect
-  publish: [/detected_objects]
-  subscribe: [/config/Yolo3, /image_raw]
+  publish: [/output_objects]
+  subscribe: [/input_image]
diff --git a/vision_darknet_detect/src/vision_darknet_detect.cpp b/vision_darknet_detect/src/vision_darknet_detect.cpp
index f56e9e3..d8492af 100644
--- a/vision_darknet_detect/src/vision_darknet_detect.cpp
+++ b/vision_darknet_detect/src/vision_darknet_detect.cpp
@@ -27,149 +27,134 @@
 #include "gencolors.cpp"
 #endif
 
-namespace darknet
-{
-    uint32_t Yolo3Detector::get_network_height()
-    {
-        return darknet_network_->h;
-    }
-    uint32_t Yolo3Detector::get_network_width()
-    {
-        return darknet_network_->w;
-    }
-    void Yolo3Detector::load(std::string& in_model_file, std::string& in_trained_file, double in_min_confidence, double in_nms_threshold)
-    {
-        min_confidence_ = in_min_confidence;
-        nms_threshold_ = in_nms_threshold;
-        darknet_network_ = parse_network_cfg(&in_model_file[0]);
-        load_weights(darknet_network_, &in_trained_file[0]);
-        set_batch_network(darknet_network_, 1);
-
-        layer output_layer = darknet_network_->layers[darknet_network_->n - 1];
-        darknet_boxes_.resize(output_layer.w * output_layer.h * output_layer.n);
-    }
-
-    Yolo3Detector::~Yolo3Detector()
-    {
-        free_network(darknet_network_);
-    }
-
-    std::vector< RectClassScore<float> > Yolo3Detector::detect(image& in_darknet_image)
-    {
-        return forward(in_darknet_image);
-    }
+namespace darknet {
+uint32_t Yolo3Detector::get_network_height() { return darknet_network_->h; }
+uint32_t Yolo3Detector::get_network_width() { return darknet_network_->w; }
+void Yolo3Detector::load(std::string& in_model_file,
+                         std::string& in_trained_file, double in_min_confidence,
+                         double in_nms_threshold) {
+    min_confidence_ = in_min_confidence;
+    nms_threshold_ = in_nms_threshold;
+    darknet_network_ = parse_network_cfg(&in_model_file[0]);
+    load_weights(darknet_network_, &in_trained_file[0]);
+    set_batch_network(darknet_network_, 1);
+
+    layer output_layer = darknet_network_->layers[darknet_network_->n - 1];
+    darknet_boxes_.resize(output_layer.w * output_layer.h * output_layer.n);
+}
 
-    image Yolo3Detector::convert_image(const sensor_msgs::ImageConstPtr& msg)
-    {
-        if (msg->encoding != sensor_msgs::image_encodings::BGR8)
-        {
-            ROS_ERROR("Unsupported encoding");
-            exit(-1);
-        }
+Yolo3Detector::~Yolo3Detector() { free_network(darknet_network_); }
 
-        auto data = msg->data;
-        uint32_t height = msg->height, width = msg->width, offset = msg->step - 3 * width;
-        uint32_t i = 0, j = 0;
-        image im = make_image(width, height, 3);
+std::vector<RectClassScore<float> > Yolo3Detector::detect(
+    image& in_darknet_image) {
+    return forward(in_darknet_image);
+}
 
-        for (uint32_t line = height; line; line--)
-        {
-            for (uint32_t column = width; column; column--)
-            {
-                for (uint32_t channel = 0; channel < 3; channel++)
-                    im.data[i + width * height * channel] = data[j++] / 255.;
-                i++;
-            }
-            j += offset;
-        }
+image Yolo3Detector::convert_image(const sensor_msgs::ImageConstPtr& msg) {
+    if (msg->encoding != sensor_msgs::image_encodings::BGR8) {
+        ROS_ERROR("Unsupported encoding");
+        exit(-1);
+    }
 
-        if (darknet_network_->w == (int) width && darknet_network_->h == (int) height)
-        {
-            return im;
+    auto data = msg->data;
+    uint32_t height = msg->height, width = msg->width,
+             offset = msg->step - 3 * width;
+    uint32_t i = 0, j = 0;
+    image im = make_image(width, height, 3);
+
+    for (uint32_t line = height; line; line--) {
+        for (uint32_t column = width; column; column--) {
+            for (uint32_t channel = 0; channel < 3; channel++)
+                im.data[i + width * height * channel] = data[j++] / 255.;
+            i++;
         }
-        image resized = resize_image(im, darknet_network_->w, darknet_network_->h);
-        free_image(im);
-        return resized;
+        j += offset;
     }
 
-    std::vector< RectClassScore<float> > Yolo3Detector::forward(image& in_darknet_image)
-    {
-        float * in_data = in_darknet_image.data;
-        float *prediction = network_predict(darknet_network_, in_data);
-        layer output_layer = darknet_network_->layers[darknet_network_->n - 1];
-
-        output_layer.output = prediction;
-        int nboxes = 0;
-        int num_classes = output_layer.classes;
-        detection *darknet_detections = get_network_boxes(darknet_network_, darknet_network_->w, darknet_network_->h, min_confidence_, .5, NULL, 0, &nboxes);
-
-        do_nms_sort(darknet_detections, nboxes, num_classes, nms_threshold_);
-
-        std::vector< RectClassScore<float> > detections;
+    if (darknet_network_->w == (int)width &&
+        darknet_network_->h == (int)height) {
+        return im;
+    }
+    image resized = resize_image(im, darknet_network_->w, darknet_network_->h);
+    free_image(im);
+    return resized;
+}
 
-        for (int i = 0; i < nboxes; i++)
-        {
-            int class_id = -1;
-            float score = 0.f;
-            //find the class
-            for(int j = 0; j < num_classes; ++j){
-                if (darknet_detections[i].prob[j] >= min_confidence_){
-                    if (class_id < 0) {
-                        class_id = j;
-                        score = darknet_detections[i].prob[j];
-                    }
+std::vector<RectClassScore<float> > Yolo3Detector::forward(
+    image& in_darknet_image) {
+    float* in_data = in_darknet_image.data;
+    float* prediction = network_predict(darknet_network_, in_data);
+    layer output_layer = darknet_network_->layers[darknet_network_->n - 1];
+
+    output_layer.output = prediction;
+    int nboxes = 0;
+    int num_classes = output_layer.classes;
+    detection* darknet_detections = get_network_boxes(
+        darknet_network_, darknet_network_->w, darknet_network_->h,
+        min_confidence_, .5, NULL, 0, &nboxes);
+
+    do_nms_sort(darknet_detections, nboxes, num_classes, nms_threshold_);
+
+    std::vector<RectClassScore<float> > detections;
+
+    for (int i = 0; i < nboxes; i++) {
+        int class_id = -1;
+        float score = 0.f;
+        // find the class
+        for (int j = 0; j < num_classes; ++j) {
+            if (darknet_detections[i].prob[j] >= min_confidence_) {
+                if (class_id < 0) {
+                    class_id = j;
+                    score = darknet_detections[i].prob[j];
                 }
             }
-            //if class found
-            if (class_id >= 0)
-            {
-                RectClassScore<float> detection;
-
-                detection.x = darknet_detections[i].bbox.x - darknet_detections[i].bbox.w/2;
-                detection.y = darknet_detections[i].bbox.y - darknet_detections[i].bbox.h/2;
-                detection.w = darknet_detections[i].bbox.w;
-                detection.h = darknet_detections[i].bbox.h;
-                detection.score = score;
-                detection.class_type = class_id;
-                //std::cout << detection.toString() << std::endl;
-
-                detections.push_back(detection);
-            }
         }
-        //std::cout << std::endl;
-        return detections;
+        // if class found
+        if (class_id >= 0) {
+            RectClassScore<float> detection;
+
+            detection.x =
+                darknet_detections[i].bbox.x - darknet_detections[i].bbox.w / 2;
+            detection.y =
+                darknet_detections[i].bbox.y - darknet_detections[i].bbox.h / 2;
+            detection.w = darknet_detections[i].bbox.w;
+            detection.h = darknet_detections[i].bbox.h;
+            detection.score = score;
+            detection.class_type = class_id;
+            // std::cout << detection.toString() << std::endl;
+
+            detections.push_back(detection);
+        }
     }
+    // std::cout << std::endl;
+    return detections;
+}
 }  // namespace darknet
 
 ///////////////////
 
-void Yolo3DetectorNode::convert_rect_to_image_obj(std::vector< RectClassScore<float> >& in_objects, autoware_msgs::DetectedObjectArray& out_message)
-{
-    for (unsigned int i = 0; i < in_objects.size(); ++i)
-    {
+void Yolo3DetectorNode::convert_rect_to_image_obj(
+    std::vector<RectClassScore<float> >& in_objects,
+    autoware_msgs::DetectedObjectArray& out_message) {
+    for (unsigned int i = 0; i < in_objects.size(); ++i) {
         {
             autoware_msgs::DetectedObject obj;
 
-            obj.x = (in_objects[i].x /image_ratio_) - image_left_right_border_/image_ratio_;
-            obj.y = (in_objects[i].y /image_ratio_) - image_top_bottom_border_/image_ratio_;
-            obj.width = in_objects[i].w /image_ratio_;
-            obj.height = in_objects[i].h /image_ratio_;
-            if (in_objects[i].x < 0)
-                obj.x = 0;
-            if (in_objects[i].y < 0)
-                obj.y = 0;
-            if (in_objects[i].w < 0)
-                obj.width = 0;
-            if (in_objects[i].h < 0)
-                obj.height = 0;
+            obj.x = (in_objects[i].x / image_ratio_) -
+                    image_left_right_border_ / image_ratio_;
+            obj.y = (in_objects[i].y / image_ratio_) -
+                    image_top_bottom_border_ / image_ratio_;
+            obj.width = in_objects[i].w / image_ratio_;
+            obj.height = in_objects[i].h / image_ratio_;
+            if (in_objects[i].x < 0) obj.x = 0;
+            if (in_objects[i].y < 0) obj.y = 0;
+            if (in_objects[i].w < 0) obj.width = 0;
+            if (in_objects[i].h < 0) obj.height = 0;
 
             obj.score = in_objects[i].score;
-            if (use_coco_names_)
-            {
+            if (use_coco_names_) {
                 obj.label = in_objects[i].GetClassString();
-            }
-            else
-            {
+            } else {
                 if (in_objects[i].class_type < custom_names_.size())
                     obj.label = custom_names_[in_objects[i].class_type];
                 else
@@ -178,56 +163,56 @@ void Yolo3DetectorNode::convert_rect_to_image_obj(std::vector< RectClassScore<fl
             obj.valid = true;
 
             out_message.objects.push_back(obj);
-
         }
     }
 }
 
-void Yolo3DetectorNode::rgbgr_image(image& im)
-{
+void Yolo3DetectorNode::rgbgr_image(image& im) {
     int i;
-    for(i = 0; i < im.w*im.h; ++i)
-    {
+    for (i = 0; i < im.w * im.h; ++i) {
         float swap = im.data[i];
-        im.data[i] = im.data[i+im.w*im.h*2];
-        im.data[i+im.w*im.h*2] = swap;
+        im.data[i] = im.data[i + im.w * im.h * 2];
+        im.data[i + im.w * im.h * 2] = swap;
     }
 }
 
-image Yolo3DetectorNode::convert_ipl_to_image(const sensor_msgs::ImageConstPtr& msg)
-{
-    cv_bridge::CvImagePtr cv_image = cv_bridge::toCvCopy(msg, "bgr8");//toCvCopy(image_source, sensor_msgs::image_encodings::BGR8);
+image Yolo3DetectorNode::convert_ipl_to_image(
+    const sensor_msgs::ImageConstPtr& msg) {
+    cv_bridge::CvImagePtr cv_image = cv_bridge::toCvCopy(
+        msg,
+        "bgr8");  // toCvCopy(image_source, sensor_msgs::image_encodings::BGR8);
     cv::Mat mat_image = cv_image->image;
 
     int network_input_width = yolo_detector_.get_network_width();
     int network_input_height = yolo_detector_.get_network_height();
 
-    int image_height = msg->height,
-            image_width = msg->width;
+    int image_height = msg->height, image_width = msg->width;
 
     IplImage ipl_image;
     cv::Mat final_mat;
 
-    if (network_input_width!=image_width
-        || network_input_height != image_height)
-    {
-        //final_mat = cv::Mat(network_input_width, network_input_height, CV_8UC3, cv::Scalar(0,0,0));
-        image_ratio_ = (double ) network_input_width /  (double)mat_image.cols;
-
-        cv::resize(mat_image, final_mat, cv::Size(), image_ratio_, image_ratio_);
-        image_top_bottom_border_ = abs(final_mat.rows-network_input_height)/2;
-        image_left_right_border_ = abs(final_mat.cols-network_input_width)/2;
-        cv::copyMakeBorder(final_mat, final_mat,
-                           image_top_bottom_border_, image_top_bottom_border_,
-                           image_left_right_border_, image_left_right_border_,
-                           cv::BORDER_CONSTANT, cv::Scalar(0,0,0));
-    }
-    else
+    if (network_input_width != image_width ||
+        network_input_height != image_height) {
+        // final_mat = cv::Mat(network_input_width, network_input_height,
+        // CV_8UC3, cv::Scalar(0,0,0));
+        image_ratio_ = (double)network_input_width / (double)mat_image.cols;
+
+        cv::resize(mat_image, final_mat, cv::Size(), image_ratio_,
+                   image_ratio_);
+        image_top_bottom_border_ =
+            abs(final_mat.rows - network_input_height) / 2;
+        image_left_right_border_ =
+            abs(final_mat.cols - network_input_width) / 2;
+        cv::copyMakeBorder(final_mat, final_mat, image_top_bottom_border_,
+                           image_top_bottom_border_, image_left_right_border_,
+                           image_left_right_border_, cv::BORDER_CONSTANT,
+                           cv::Scalar(0, 0, 0));
+    } else
         final_mat = mat_image;
 
     ipl_image = final_mat;
 
-    unsigned char *data = (unsigned char *)ipl_image.imageData;
+    unsigned char* data = (unsigned char*)ipl_image.imageData;
     int h = ipl_image.height;
     int w = ipl_image.width;
     int c = ipl_image.nChannels;
@@ -236,10 +221,11 @@ image Yolo3DetectorNode::convert_ipl_to_image(const sensor_msgs::ImageConstPtr&
 
     image darknet_image = make_image(w, h, c);
 
-    for(i = 0; i < h; ++i){
-        for(k= 0; k < c; ++k){
-            for(j = 0; j < w; ++j){
-                darknet_image.data[k*w*h + i*w + j] = data[i*step + j*c + k]/255.;
+    for (i = 0; i < h; ++i) {
+        for (k = 0; k < c; ++k) {
+            for (j = 0; j < w; ++j) {
+                darknet_image.data[k * w * h + i * w + j] =
+                    data[i * step + j * c + k] / 255.;
             }
         }
     }
@@ -247,15 +233,15 @@ image Yolo3DetectorNode::convert_ipl_to_image(const sensor_msgs::ImageConstPtr&
     return darknet_image;
 }
 
-void Yolo3DetectorNode::image_callback(const sensor_msgs::ImageConstPtr& in_image_message)
-{
-    std::vector< RectClassScore<float> > detections;
+void Yolo3DetectorNode::image_callback(
+    const sensor_msgs::ImageConstPtr& in_image_message) {
+    std::vector<RectClassScore<float> > detections;
 
     darknet_image_ = convert_ipl_to_image(in_image_message);
 
     detections = yolo_detector_.detect(darknet_image_);
 
-    //Prepare Output message
+    // Prepare Output message
     autoware_msgs::DetectedObjectArray output_message;
     output_message.header = in_image_message->header;
 
@@ -266,103 +252,81 @@ void Yolo3DetectorNode::image_callback(const sensor_msgs::ImageConstPtr& in_imag
     free(darknet_image_.data);
 }
 
-void Yolo3DetectorNode::config_cb(const autoware_config_msgs::ConfigSSD::ConstPtr& param)
-{
-    score_threshold_ = param->score_threshold;
-}
-
-std::vector<std::string> Yolo3DetectorNode::read_custom_names_file(const std::string& in_names_path)
-{
+std::vector<std::string> Yolo3DetectorNode::read_custom_names_file(
+    const std::string& in_names_path) {
     std::ifstream file(in_names_path);
     std::string str;
     std::vector<std::string> names;
-    while (std::getline(file, str))
-    {
+    while (std::getline(file, str)) {
         names.push_back(str);
-        std::cout << str <<  std::endl;
+        std::cout << str << std::endl;
     }
     return names;
 }
 
-void Yolo3DetectorNode::Run()
-{
-    //ROS STUFF
-    ros::NodeHandle private_node_handle("~");//to receive args
+void Yolo3DetectorNode::Run() {
+    // ROS STUFF
+    ros::NodeHandle private_node_handle("~");  // to receive args
 
-    //RECEIVE IMAGE TOPIC NAME
-    std::string image_raw_topic_str;
-    if (private_node_handle.getParam("image_raw_node", image_raw_topic_str))
-    {
-        ROS_INFO("Setting image node to %s", image_raw_topic_str.c_str());
-    }
-    else
-    {
-        ROS_INFO("No image node received, defaulting to /image_raw, you can use _image_raw_node:=YOUR_TOPIC");
-        image_raw_topic_str = "/image_raw";
-    }
+    std::string input_image_str = "/input_image";
+    std::string output_objects_str = "/output_objects";
 
     std::string network_definition_file;
     std::string pretrained_model_file, names_file;
-    if (private_node_handle.getParam("network_definition_file", network_definition_file))
-    {
-        ROS_INFO("Network Definition File (Config): %s", network_definition_file.c_str());
-    }
-    else
-    {
-        ROS_INFO("No Network Definition File was received. Finishing execution.");
+    if (private_node_handle.getParam("network_definition_file",
+                                     network_definition_file)) {
+        ROS_INFO("Network Definition File (Config): %s",
+                 network_definition_file.c_str());
+    } else {
+        ROS_INFO(
+            "No Network Definition File was received. Finishing execution.");
         return;
     }
-    if (private_node_handle.getParam("pretrained_model_file", pretrained_model_file))
-    {
-        ROS_INFO("Pretrained Model File (Weights): %s", pretrained_model_file.c_str());
-    }
-    else
-    {
+    if (private_node_handle.getParam("pretrained_model_file",
+                                     pretrained_model_file)) {
+        ROS_INFO("Pretrained Model File (Weights): %s",
+                 pretrained_model_file.c_str());
+    } else {
         ROS_INFO("No Pretrained Model File was received. Finishing execution.");
         return;
     }
 
-    if (private_node_handle.getParam("names_file", names_file))
-    {
+    if (private_node_handle.getParam("names_file", names_file)) {
         ROS_INFO("Names File: %s", names_file.c_str());
         use_coco_names_ = false;
         custom_names_ = read_custom_names_file(names_file);
-    }
-    else
-    {
+    } else {
         ROS_INFO("No Names file was received. Using default COCO names.");
         use_coco_names_ = true;
     }
 
-    private_node_handle.param<float>("score_threshold", score_threshold_, 0.5);
-    ROS_INFO("[%s] score_threshold: %f",__APP_NAME__, score_threshold_);
+    private_node_handle.param<float>("score_threshold_darknet",
+                                     score_threshold_, 0.5);
+    ROS_INFO("[%s] score_threshold: %f", __APP_NAME__, score_threshold_);
 
     private_node_handle.param<float>("nms_threshold", nms_threshold_, 0.45);
-    ROS_INFO("[%s] nms_threshold: %f",__APP_NAME__, nms_threshold_);
-
+    ROS_INFO("[%s] nms_threshold: %f", __APP_NAME__, nms_threshold_);
 
     ROS_INFO("Initializing Yolo on Darknet...");
-    yolo_detector_.load(network_definition_file, pretrained_model_file, score_threshold_, nms_threshold_);
+    yolo_detector_.load(network_definition_file, pretrained_model_file,
+                        score_threshold_, nms_threshold_);
     ROS_INFO("Initialization complete.");
 
-    #if (CV_MAJOR_VERSION <= 2)
-        cv::generateColors(colors_, 80);
-    #else
-        generateColors(colors_, 80);
-    #endif
-
-    publisher_objects_ = node_handle_.advertise<autoware_msgs::DetectedObjectArray>("/detection/image_detector/objects", 1);
+#if (CV_MAJOR_VERSION <= 2)
+    cv::generateColors(colors_, 80);
+#else
+    generateColors(colors_, 80);
+#endif
 
-    ROS_INFO("Subscribing to... %s", image_raw_topic_str.c_str());
-    subscriber_image_raw_ = node_handle_.subscribe(image_raw_topic_str, 1, &Yolo3DetectorNode::image_callback, this);
+    publisher_objects_ =
+        node_handle_.advertise<autoware_msgs::DetectedObjectArray>(
+            output_objects_str, 1);
 
-    std::string config_topic("/config");
-    config_topic += "/Yolo3";
-    subscriber_yolo_config_ = node_handle_.subscribe(config_topic, 1, &Yolo3DetectorNode::config_cb, this);
+    subscriber_image_raw_ = node_handle_.subscribe(
+        input_image_str, 1, &Yolo3DetectorNode::image_callback, this);
 
-    ROS_INFO_STREAM( __APP_NAME__ << "" );
+    ROS_INFO_STREAM(__APP_NAME__ << "");
 
     ros::spin();
     ROS_INFO("END Yolo");
-
 }
diff --git a/vision_darknet_detect/src/vision_darknet_detect.h b/vision_darknet_detect/src/vision_darknet_detect.h
index 6fb5cf8..831d6bf 100644
--- a/vision_darknet_detect/src/vision_darknet_detect.h
+++ b/vision_darknet_detect/src/vision_darknet_detect.h
@@ -111,7 +111,6 @@ class Yolo3DetectorNode {
     void                            rgbgr_image(image& im);
     image                           convert_ipl_to_image(const sensor_msgs::ImageConstPtr& msg);
     void                            image_callback(const sensor_msgs::ImageConstPtr& in_image_message);
-    void                            config_cb(const autoware_config_msgs::ConfigSSD::ConstPtr& param);
     std::vector<std::string>        read_custom_names_file(const std::string& in_path);
 public:
     void    Run();
